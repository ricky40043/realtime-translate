<template>
  <div class="smart-voice-recorder">
    <button
      @click="toggleRecording"
      :class="['smart-record-btn', { 
        recording: isRecording, 
        processing: isProcessing,
        disabled: !isSupported || isProcessing 
      }]"
      :disabled="!isSupported || isProcessing"
    >
      <div class="btn-content">
        <div class="icon">
          <span v-if="!isRecording && !isProcessing">ğŸ¤</span>
          <span v-else-if="isProcessing">â³</span>
          <div v-else class="recording-animation">
            <div class="pulse"></div>
            <span>ğŸ”´</span>
          </div>
        </div>
        <div class="text">
          <span v-if="!isSupported">ä¸æ”¯æ´</span>
          <span v-else-if="isProcessing">è™•ç†ä¸­...</span>
          <span v-else-if="isRecording">
            {{ recordingMode === 'smart' ? 'æ™ºèƒ½éŒ„éŸ³ä¸­' : 'æ‰‹å‹•éŒ„éŸ³ä¸­' }} {{ formatTime(recordingTime) }}
            <br>
            <small>{{ vadStatus || 'é»æ“Šå¯åœæ­¢éŒ„éŸ³' }}</small>
          </span>
          <span v-else>{{ recordingMode === 'manual' ? 'é»æ“Šé–‹å§‹éŒ„éŸ³' : 'æ™ºèƒ½èªéŸ³è¼¸å…¥' }}</span>
        </div>
      </div>
    </button>
    
    <!-- æ™ºèƒ½æª¢æ¸¬ç‹€æ…‹æŒ‡ç¤ºå™¨ -->
    <div v-if="isRecording" class="smart-indicator">
      <div class="volume-display">
        <div class="volume-bars">
          <div 
            v-for="i in 10" 
            :key="i"
            class="volume-bar"
            :class="{ 
              active: volumeLevel >= i,
              voice: isVoiceDetected && volumeLevel >= i,
              silence: !isVoiceDetected && volumeLevel >= i
            }"
          ></div>
        </div>
        <div class="threshold-line" :style="{ left: `${(settings.voiceThreshold / 50) * 100}%` }"></div>
      </div>
      
      <div class="vad-info">
        <div class="vad-status" :class="{ active: isVoiceDetected }">
          {{ isVoiceDetected ? 'ğŸ™ï¸ æª¢æ¸¬åˆ°èªéŸ³' : 'ğŸ”‡ éœéŸ³ä¸­' }}
        </div>
        <div class="silence-timer" v-if="!isVoiceDetected && silenceTimer > 0">
          éœéŸ³ {{ (settings.silenceTimeout - silenceTimer).toFixed(1) }}s
        </div>
        <div class="auto-stop-info" v-if="recordingTime >= settings.maxRecordingTime - 5">
          å°‡åœ¨ {{ (settings.maxRecordingTime - recordingTime).toFixed(0) }}s å¾Œè‡ªå‹•çµæŸ
        </div>
      </div>
    </div>
    
    <!-- éŒ„éŸ³æ¨¡å¼åˆ‡æ› -->
    <div class="mode-selector">
      <button 
        @click="recordingMode = 'smart'"
        :class="['mode-btn', { active: recordingMode === 'smart' }]"
      >
        ğŸ§  æ™ºèƒ½æ¨¡å¼
      </button>
      <button 
        @click="recordingMode = 'manual'"
        :class="['mode-btn', { active: recordingMode === 'manual' }]"
      >
        ğŸ‘† æ‰‹å‹•æ¨¡å¼
      </button>
    </div>
    
    <!-- éŒ¯èª¤è¨Šæ¯ -->
    <div v-if="error" class="error-message">
      {{ error }}
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted, onUnmounted, watch } from 'vue'
import { speechApi } from '../api/speech'

interface SmartSettings {
  voiceThreshold: number
  silenceTimeout: number
  minRecordingTime: number
  maxRecordingTime: number
}

interface Props {
  roomId: string
  disabled?: boolean
  userLang?: string
  settings: SmartSettings
}

interface Emits {
  (e: 'transcript', result: { text: string; confidence: number; lang: string }): void
  (e: 'error', error: string): void
  (e: 'recording-start'): void
  (e: 'recording-end'): void
}

const props = defineProps<Props>()
const emit = defineEmits<Emits>()

// éŸ¿æ‡‰å¼ç‹€æ…‹
const isSupported = ref(false)
const isRecording = ref(false)
const isProcessing = ref(false)
const recordingTime = ref(0)
const volumeLevel = ref(0)
const error = ref('')
const recordingMode = ref<'smart' | 'manual'>('smart')

// èªéŸ³æ´»å‹•æª¢æ¸¬ (VAD) ç›¸é—œ
const isVoiceDetected = ref(false)
const vadStatus = ref('ç­‰å¾…èªéŸ³...')
const silenceTimer = ref(0)
const voiceStartTime = ref(0)
const hasValidSpeech = ref(false)
const currentVolume = ref(0) // ç•¶å‰éŸ³é‡ç™¾åˆ†æ¯”

// è‡ªå‹•åˆ†æ®µéŒ„éŸ³ç›¸é—œ
const segmentTimer = ref(0)
const isSegmentMode = ref(false)
const segmentThreshold = 10 // 10%éŸ³é‡é–¾å€¼ç”¨æ–¼åˆ†æ®µ
const minSegmentTime = 1.0 // æœ€å°‘éŒ„éŸ³1ç§’æ‰èƒ½åˆ†æ®µ
const hasProcessedSegment = ref(false)

// åª’é«”ç›¸é—œ
const mediaRecorder = ref<MediaRecorder | null>(null)
const audioChunks = ref<Blob[]>([])
const stream = ref<MediaStream | null>(null)
const recordingTimer = ref<number | null>(null)
const volumeAnalyser = ref<AnalyserNode | null>(null)
const volumeAnimationFrame = ref<number | null>(null)
const vadTimer = ref<number | null>(null)

// æ™ºèƒ½æª¢æ¸¬åƒæ•¸
const vadSamples = ref<number[]>([])
const vadSampleSize = 5 // å–æ¨£æ•¸é‡ç”¨æ–¼å¹³æ»‘è™•ç†

onMounted(() => {
  checkSupport()
})

onUnmounted(() => {
  fullCleanup()
})

// ç›£è½è¨­å®šè®ŠåŒ–
watch(() => props.settings, () => {
  console.log('ğŸ”§ èªéŸ³è¨­å®šå·²æ›´æ–°:', props.settings)
}, { deep: true })

function checkSupport() {
  isSupported.value = !!(
    navigator.mediaDevices &&
    navigator.mediaDevices.getUserMedia &&
    window.MediaRecorder
  )
}

async function toggleRecording() {
  if (isRecording.value) {
    // æ™ºèƒ½æ¨¡å¼å’Œæ‰‹å‹•æ¨¡å¼éƒ½å…è¨±æ‰‹å‹•åœæ­¢
    await stopRecording()
  } else {
    await startRecording()
  }
}

async function startRecording() {
  try {
    error.value = ''
    console.log('ğŸ¤ é–‹å§‹è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™...')
    
    // æª¢æŸ¥ç€è¦½å™¨æ˜¯å¦æ”¯æ´
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('ç€è¦½å™¨ä¸æ”¯æ´éº¥å…‹é¢¨åŠŸèƒ½')
    }
    
    // è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™
    stream.value = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 48000
      }
    })
    
    console.log('âœ… éº¥å…‹é¢¨æ¬Šé™ç²å¾—æˆåŠŸ')
    console.log('ğŸ¤ éŸ³é »è»Œé“ç‹€æ…‹:', stream.value.getAudioTracks().map(track => ({
      label: track.label,
      enabled: track.enabled,
      readyState: track.readyState,
      settings: track.getSettings()
    })))
    
    // è¨­å®šæ™ºèƒ½èªéŸ³æª¢æ¸¬
    setupSmartVAD(stream.value)
    
    // å»ºç«‹ MediaRecorder
    const mimeType = getSupportedMimeType()
    mediaRecorder.value = new MediaRecorder(stream.value, {
      mimeType: mimeType,
      audioBitsPerSecond: 128000
    })
    
    audioChunks.value = []
    
    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data)
      }
    }
    
    mediaRecorder.value.onstop = async () => {
      await processRecording()
    }
    
    // é–‹å§‹éŒ„éŸ³
    mediaRecorder.value.start(100)
    isRecording.value = true
    recordingTime.value = 0
    silenceTimer.value = 0
    hasValidSpeech.value = false
    
    // å•Ÿå‹•éŸ³é »åˆ†æ
    console.log('ğŸš€ éŒ„éŸ³å·²é–‹å§‹ï¼Œç¾åœ¨å•Ÿå‹•éŸ³é »åˆ†æ...')
    startAudioAnalysis()
    
    // é–‹å§‹è¨ˆæ™‚
    recordingTimer.value = window.setInterval(() => {
      recordingTime.value += 0.1
      
      // æª¢æŸ¥æœ€é•·éŒ„éŸ³æ™‚é–“
      if (recordingTime.value >= props.settings.maxRecordingTime) {
        vadStatus.value = 'é”åˆ°æœ€é•·éŒ„éŸ³æ™‚é–“'
        stopRecording()
        return
      }
      
      // æ™ºèƒ½æ¨¡å¼ä¸‹çš„è‡ªå‹•åœæ­¢é‚è¼¯
      if (recordingMode.value === 'smart') {
        handleSmartRecordingLogic().catch(error => {
          console.error('âŒ æ™ºèƒ½éŒ„éŸ³é‚è¼¯éŒ¯èª¤:', error)
        })
      }
    }, 100)
    
    emit('recording-start')
    console.log('ğŸ¤ é–‹å§‹æ™ºèƒ½éŒ„éŸ³')
    
  } catch (err) {
    console.error('éŒ„éŸ³å¤±æ•—:', err)
    error.value = 'ç„¡æ³•å­˜å–éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥æ¬Šé™è¨­å®š'
    cleanup()
  }
}

async function handleSmartRecordingLogic() {
  // åœ¨æ™ºèƒ½æ¨¡å¼ä¸‹æ‰åŸ·è¡Œè‡ªå‹•é‚è¼¯
  if (recordingMode.value !== 'smart') return
  
  // ä½¿ç”¨å¯¦éš›éŸ³é‡æ•¸å€¼é€²è¡Œåˆ¤æ–·
  const currentVol = currentVolume.value
  const isLowVolume = currentVol <= segmentThreshold // ä½æ–¼10%
  
  // æª¢æŸ¥è‡ªå‹•åˆ†æ®µé‚è¼¯ï¼ˆå„ªå…ˆè™•ç†ï¼‰
  if (recordingTime.value >= minSegmentTime && isLowVolume) {
    segmentTimer.value += 0.1
    
    // å¦‚æœå·²ç¶“éŒ„éŸ³è¶…é1ç§’ï¼Œä¸”éŸ³é‡æŒçºŒä½æ–¼10%ï¼Œå‰‡é€²è¡Œåˆ†æ®µ
    if (segmentTimer.value >= 0.3 && hasValidSpeech.value) {
      vadStatus.value = 'æª¢æ¸¬åˆ°åˆ†æ®µé»ï¼Œé€å‡ºéŸ³æª”...'
      console.log(`ğŸµ è‡ªå‹•åˆ†æ®µï¼šéŒ„éŸ³ ${recordingTime.value.toFixed(1)}sï¼ŒéŸ³é‡ ${currentVol.toFixed(1)}% ä½æ–¼ ${segmentThreshold}% æŒçºŒ ${segmentTimer.value.toFixed(1)}s`)
      await processCurrentSegment()
      return
    }
  } else if (!isLowVolume) {
    // æª¢æ¸¬åˆ°èªéŸ³ï¼Œé‡ç½®åˆ†æ®µè¨ˆæ™‚å™¨
    segmentTimer.value = 0
  }
  
  // åŸæœ‰çš„çµæŸé‚è¼¯ - ä½¿ç”¨å¯¦éš›éŸ³é‡åˆ¤æ–·
  if (isLowVolume) {
    silenceTimer.value += 0.1
    
    // æª¢æŸ¥æ˜¯å¦é”åˆ°éœéŸ³è¶…æ™‚æ™‚é–“ï¼ˆå®Œå…¨çµæŸéŒ„éŸ³ï¼‰
    if (silenceTimer.value >= props.settings.silenceTimeout) {
      // å¦‚æœæœ‰éæœ‰æ•ˆèªéŸ³ï¼Œè‡ªå‹•åœæ­¢
      if (hasValidSpeech.value) {
        vadStatus.value = 'éœéŸ³æ™‚é–“é”åˆ°é–¾å€¼ï¼Œè‡ªå‹•çµæŸ'
        console.log(`ğŸ”‡ éŸ³é‡ ${currentVol.toFixed(1)}% éœéŸ³ ${silenceTimer.value.toFixed(1)}s é”åˆ°é–¾å€¼ ${props.settings.silenceTimeout}sï¼Œå®Œå…¨çµæŸéŒ„éŸ³`)
        stopRecording()
        return
      } else if (silenceTimer.value >= props.settings.silenceTimeout * 2) {
        // å¦‚æœä¸€ç›´æ²’æœ‰æœ‰æ•ˆèªéŸ³ï¼Œå»¶é•·ä¸€å€æ™‚é–“å¾Œåœæ­¢
        vadStatus.value = 'æœªæª¢æ¸¬åˆ°æœ‰æ•ˆèªéŸ³ï¼Œè‡ªå‹•çµæŸ'
        console.log(`ğŸ”‡ æŒçºŒéœéŸ³ ${silenceTimer.value.toFixed(1)}sï¼Œæœªæª¢æ¸¬åˆ°æœ‰æ•ˆèªéŸ³ï¼Œè‡ªå‹•çµæŸ`)
        stopRecording()
        return
      }
    }
    
    vadStatus.value = hasValidSpeech.value ? 
      `éœéŸ³ä¸­(${currentVol.toFixed(1)}%)ï¼Œ${Math.max(0, props.settings.silenceTimeout - silenceTimer.value).toFixed(1)}så¾ŒçµæŸ` :
      `ç­‰å¾…èªéŸ³è¼¸å…¥(${currentVol.toFixed(1)}%)... ${Math.max(0, props.settings.silenceTimeout * 2 - silenceTimer.value).toFixed(1)}s`
  } else {
    // æª¢æ¸¬åˆ°èªéŸ³ï¼Œé‡ç½®éœéŸ³è¨ˆæ™‚å™¨
    silenceTimer.value = 0
    
    // æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€çŸ­éŒ„éŸ³æ™‚é–“
    if (recordingTime.value >= props.settings.minRecordingTime) {
      hasValidSpeech.value = true
    }
    
    vadStatus.value = hasValidSpeech.value ? `æ­£åœ¨éŒ„è£½èªéŸ³(${currentVol.toFixed(1)}%)...` : `æª¢æ¸¬ä¸­(${currentVol.toFixed(1)}%)...`
  }
}

async function stopRecording() {
  if (mediaRecorder.value && isRecording.value) {
    console.log('â¹ï¸ åœæ­¢éŒ„éŸ³')
    mediaRecorder.value.stop()
    isRecording.value = false
    
    if (recordingTimer.value) {
      clearInterval(recordingTimer.value)
      recordingTimer.value = null
    }
    
    stopVolumeAnalysis()
    emit('recording-end')
  }
}

// è™•ç†è‡ªå‹•åˆ†æ®µ
async function processCurrentSegment() {
  if (!mediaRecorder.value || audioChunks.value.length === 0) {
    console.log('âš ï¸ ç„¡éŸ³æª”è³‡æ–™å¯åˆ†æ®µ')
    return
  }
  
  try {
    // æš«åœç•¶å‰éŒ„éŸ³å™¨ï¼ˆä¿æŒstreamæ´»èºï¼‰
    mediaRecorder.value.stop()
    
    // ç­‰å¾…ondataavailableäº‹ä»¶å®Œæˆ
    await new Promise(resolve => {
      if (mediaRecorder.value) {
        mediaRecorder.value.onstop = resolve
      } else {
        resolve(undefined)
      }
    })
    
    // è™•ç†ç•¶å‰éŸ³æª”æ®µè½
    const segmentChunks = [...audioChunks.value]
    audioChunks.value = [] // æ¸…ç©ºæº–å‚™ä¸‹ä¸€æ®µ
    
    if (segmentChunks.length > 0) {
      console.log('ğŸµ è™•ç†åˆ†æ®µéŸ³æª”...')
      const mimeType = getSupportedMimeType()
      const segmentBlob = new Blob(segmentChunks, { type: mimeType })
      
      // ç•°æ­¥ä¸Šå‚³ç•¶å‰æ®µè½
      uploadSegmentAudio(segmentBlob)
    }
    
    // é‡æ–°é–‹å§‹éŒ„éŸ³ä¸‹ä¸€æ®µ
    await restartRecordingForNextSegment()
    
  } catch (error) {
    console.error('âŒ åˆ†æ®µè™•ç†å¤±æ•—:', error)
    // å¦‚æœåˆ†æ®µå¤±æ•—ï¼Œç¹¼çºŒæ­£å¸¸éŒ„éŸ³
    await restartRecordingForNextSegment()
  }
}

// é‡æ–°é–‹å§‹éŒ„éŸ³ä¸‹ä¸€æ®µ
async function restartRecordingForNextSegment() {
  if (!stream.value) return
  
  try {
    // å»ºç«‹æ–°çš„ MediaRecorder
    const mimeType = getSupportedMimeType()
    mediaRecorder.value = new MediaRecorder(stream.value, {
      mimeType: mimeType,
      audioBitsPerSecond: 128000
    })
    
    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data)
      }
    }
    
    mediaRecorder.value.onstop = async () => {
      await processRecording()
    }
    
    // é‡æ–°é–‹å§‹éŒ„éŸ³
    mediaRecorder.value.start(100)
    
    // é‡ç½®åˆ†æ®µç›¸é—œç‹€æ…‹
    segmentTimer.value = 0
    hasValidSpeech.value = false
    
    console.log('ğŸ¤ é–‹å§‹éŒ„éŸ³æ–°æ®µè½')
    
  } catch (error) {
    console.error('âŒ é‡æ–°é–‹å§‹éŒ„éŸ³å¤±æ•—:', error)
    await stopRecording()
  }
}

// ç•°æ­¥ä¸Šå‚³åˆ†æ®µéŸ³æª”
async function uploadSegmentAudio(audioBlob: Blob) {
  try {
    console.log(`ğŸ“¤ ä¸Šå‚³åˆ†æ®µéŸ³æª”ï¼Œå¤§å°: ${(audioBlob.size / 1024).toFixed(1)} KB`)
    
    const result = await speechApi.upload(props.roomId, audioBlob, props.userLang)
    console.log('âœ… åˆ†æ®µéŸ³æª”STTæˆåŠŸ:', result)
    
    emit('transcript', {
      text: result.transcript,
      confidence: result.confidence,
      lang: result.detected_lang
    })
    
  } catch (error) {
    console.error('âŒ åˆ†æ®µéŸ³æª”ä¸Šå‚³å¤±æ•—:', error)
    emit('error', 'åˆ†æ®µéŸ³æª”è™•ç†å¤±æ•—')
  }
}

// éŸ³é »åˆ†æç›¸é—œè®Šé‡
let debugCounter = 0

function setupSmartVAD(stream: MediaStream) {
  try {
    console.log('ğŸ”§ é–‹å§‹è¨­å®šèªéŸ³æª¢æ¸¬...')
    
    const audioContext = new AudioContext()
    console.log('ğŸµ AudioContext ç‹€æ…‹:', audioContext.state)
    
    // å¦‚æœAudioContextè¢«æš«åœï¼Œå˜—è©¦æ¢å¾©
    if (audioContext.state === 'suspended') {
      audioContext.resume().then(() => {
        console.log('ğŸµ AudioContext å·²æ¢å¾©')
      })
    }
    
    const source = audioContext.createMediaStreamSource(stream)
    volumeAnalyser.value = audioContext.createAnalyser()
    
    // èª¿æ•´è¨­å®šä»¥ç²å¾—æ›´å¥½çš„èªéŸ³æª¢æ¸¬
    volumeAnalyser.value.fftSize = 256
    volumeAnalyser.value.smoothingTimeConstant = 0.8
    
    source.connect(volumeAnalyser.value)
    console.log('ğŸ”— éŸ³é »æºå·²é€£æ¥åˆ°åˆ†æå™¨')
    
    console.log('âœ… èªéŸ³æª¢æ¸¬è¨­å®šå®Œæˆï¼Œç­‰å¾…éŒ„éŸ³é–‹å§‹å¾Œå•Ÿå‹•åˆ†æ')
    
  } catch (err) {
    console.error('âŒ ç„¡æ³•è¨­å®šèªéŸ³æª¢æ¸¬:', err)
    error.value = 'èªéŸ³æª¢æ¸¬è¨­å®šå¤±æ•—ï¼š' + err.message
  }
}

function startAudioAnalysis() {
  if (!volumeAnalyser.value) {
    console.error('âŒ ç„¡æ³•å•Ÿå‹•éŸ³é »åˆ†æï¼šåˆ†æå™¨æœªåˆå§‹åŒ–')
    return
  }
  
  debugCounter = 0 // é‡ç½®è¨ˆæ•¸å™¨
  console.log('ğŸ¯ å•Ÿå‹•éŸ³é »åˆ†æå¾ªç’°...')
  
  const analyzeAudio = () => {
    // æ·»åŠ é€²å…¥å‡½æ•¸çš„LOG
    console.log(`ğŸ¯ analyzeAudio å‡½æ•¸åŸ·è¡Œä¸­... éŒ„éŸ³ç‹€æ…‹: ${isRecording.value}, åˆ†æå™¨ç‹€æ…‹: ${!!volumeAnalyser.value}`)
    
    if (!volumeAnalyser.value || !isRecording.value) {
      console.log(`âš ï¸ analyzeAudio æå‰è¿”å›: åˆ†æå™¨=${!!volumeAnalyser.value}, éŒ„éŸ³ä¸­=${isRecording.value}`)
      return
    }
    
    const dataArray = new Uint8Array(volumeAnalyser.value.frequencyBinCount)
    volumeAnalyser.value.getByteFrequencyData(dataArray)
    
    // ä½¿ç”¨åŸæœ¬æœ‰æ•ˆçš„éŸ³é‡è¨ˆç®—æ–¹å¼
    const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length
    const normalizedVolume = Math.min(100, (average / 255) * 100)
    
    // èª¿è©¦ï¼šæ¯50å¹€è¼¸å‡ºä¸€æ¬¡éŸ³é‡ä¿¡æ¯
    debugCounter++
    if (debugCounter % 50 === 0) {
      const threshold = props.settings.voiceThreshold
      console.log(`ğŸ”Š åŸå§‹éŸ³é‡: ${average.toFixed(1)}, æ¨™æº–åŒ–éŸ³é‡: ${normalizedVolume.toFixed(1)}%, é–¾å€¼: ${threshold}%, æ•¸æ“šç¯„åœ: ${Math.min(...dataArray)}-${Math.max(...dataArray)}`)
    }
    
    // æ¯5å¹€è¼¸å‡ºåŸºæœ¬ç‹€æ…‹ç¢ºèªanalyzeAudioåœ¨é‹è¡Œ
    if (debugCounter % 5 === 0) {
      console.log(`ğŸ”„ analyzeAudio ç¬¬${debugCounter}å¹€: å¹³å‡éŸ³é‡=${average.toFixed(1)}, æ¨™æº–åŒ–=${normalizedVolume.toFixed(1)}%`)
    }
    
    // å¹³æ»‘è™•ç† - æ·»åŠ è©³ç´°LOG
    vadSamples.value.push(normalizedVolume)
    if (vadSamples.value.length > vadSampleSize) {
      vadSamples.value.shift()
    }
    
    const smoothedVolume = vadSamples.value.reduce((sum, val) => sum + val, 0) / vadSamples.value.length
    
    // è©³ç´°èª¿è©¦LOG - æ¯20å¹€è¼¸å‡ºå¹³æ»‘è™•ç†è©³æƒ…
    if (debugCounter % 20 === 0) {
      console.log(`ğŸ” å¹³æ»‘è™•ç†è©³æƒ…:`)
      console.log(`  - åŸå§‹éŸ³é‡: ${normalizedVolume.toFixed(2)}%`)
      console.log(`  - æ¨£æœ¬æ•¸çµ„: [${vadSamples.value.map(v => v.toFixed(1)).join(', ')}]`)
      console.log(`  - æ¨£æœ¬æ•¸é‡: ${vadSamples.value.length}/${vadSampleSize}`)
      console.log(`  - æ¨£æœ¬ç¸½å’Œ: ${vadSamples.value.reduce((sum, val) => sum + val, 0).toFixed(2)}`)
      console.log(`  - å¹³æ»‘å¾ŒéŸ³é‡: ${smoothedVolume.toFixed(2)}%`)
      console.log(`  - éŸ³é‡æ¢ç­‰ç´š: ${volumeLevel.value}/10`)
    }
    volumeLevel.value = Math.min(10, Math.floor(smoothedVolume / 10))
    
    // æ›´æ–°ç•¶å‰éŸ³é‡
    currentVolume.value = smoothedVolume
    
    // èªéŸ³æ´»å‹•æª¢æ¸¬ - ä½¿ç”¨è¨­å®šçš„é–¾å€¼
    const threshold = props.settings.voiceThreshold
    const wasVoiceDetected = isVoiceDetected.value
    isVoiceDetected.value = smoothedVolume > threshold
    
    // èª¿è©¦ï¼šèªéŸ³æª¢æ¸¬ç‹€æ…‹æ”¹è®Šæ™‚è¼¸å‡º
    if (wasVoiceDetected !== isVoiceDetected.value) {
      console.log(`ğŸ™ï¸ èªéŸ³æª¢æ¸¬ç‹€æ…‹è®ŠåŒ–: ${isVoiceDetected.value ? 'æª¢æ¸¬åˆ°èªéŸ³' : 'éœéŸ³'} (å¹³æ»‘éŸ³é‡: ${smoothedVolume.toFixed(1)}%, è¨­å®šé–¾å€¼: ${threshold}%)`)
    }
    
    // é¡å¤–èª¿è©¦ï¼šæ¯100å¹€è¼¸å‡ºç•¶å‰æª¢æ¸¬ç‹€æ…‹å’Œåˆ†æ®µç‹€æ…‹
    if (debugCounter % 100 === 0) {
      const segmentInfo = recordingTime.value >= minSegmentTime ? 
        `åˆ†æ®µè¨ˆæ™‚: ${segmentTimer.value.toFixed(1)}s` : 
        'æœªé”åˆ†æ®µæ™‚é–“'
      console.log(`ğŸ“Š ç•¶å‰ç‹€æ…‹: éŸ³é‡=${smoothedVolume.toFixed(1)}%, é–¾å€¼=${threshold}%, æª¢æ¸¬=${isVoiceDetected.value ? 'æœ‰èªéŸ³' : 'éœéŸ³'}, éŒ„éŸ³æ™‚é–“=${recordingTime.value.toFixed(1)}s, ${segmentInfo}`)
    }
    
    volumeAnimationFrame.value = requestAnimationFrame(analyzeAudio)
    
    // ç¢ºèªä¸‹ä¸€å¹€å·²å®‰æ’
    if (debugCounter % 10 === 0) {
      console.log(`ğŸ”„ å·²å®‰æ’ä¸‹ä¸€å¹€åˆ†æ: ${volumeAnimationFrame.value}`)
    }
  }
  
  // é–‹å§‹åˆ†æ
  analyzeAudio()
}

function stopVolumeAnalysis() {
  if (volumeAnimationFrame.value) {
    cancelAnimationFrame(volumeAnimationFrame.value)
    volumeAnimationFrame.value = null
  }
  volumeLevel.value = 0
  isVoiceDetected.value = false
  vadSamples.value = []
  volumeAnalyser.value = null
}

async function processRecording() {
  if (audioChunks.value.length === 0) {
    error.value = 'éŒ„éŸ³è³‡æ–™ç‚ºç©º'
    cleanup()
    return
  }
  
  // æª¢æŸ¥æœ€çŸ­éŒ„éŸ³æ™‚é–“ - ä½†åªåœ¨æ™ºèƒ½æ¨¡å¼ä¸‹æª¢æŸ¥ï¼Œæ‰‹å‹•æ¨¡å¼ä¸é™åˆ¶
  if (recordingMode.value === 'smart' && recordingTime.value < props.settings.minRecordingTime) {
    console.log(`âš ï¸ æ™ºèƒ½æ¨¡å¼éŒ„éŸ³æ™‚é–“éçŸ­ (${recordingTime.value.toFixed(1)}s < ${props.settings.minRecordingTime}s)ï¼Œå¿½ç•¥æ­¤æ¬¡éŒ„éŸ³`)
    cleanup()
    return
  }
  
  console.log(`âœ… éŒ„éŸ³æ™‚é–“ç¬¦åˆè¦æ±‚ (${recordingTime.value.toFixed(1)}s)ï¼Œé–‹å§‹è™•ç†éŸ³æª”`)
  
  try {
    isProcessing.value = true
    console.log('ğŸ”„ è™•ç†éŒ„éŸ³è³‡æ–™...')
    
    const mimeType = getSupportedMimeType()
    const audioBlob = new Blob(audioChunks.value, { type: mimeType })
    
    console.log(`ğŸ“¦ éŸ³é »è³‡æ–™å¤§å°: ${(audioBlob.size / 1024).toFixed(1)} KBï¼Œæ™‚é•·: ${recordingTime.value.toFixed(1)}s`)
    
    await uploadAudio(audioBlob)
    
  } catch (err) {
    console.error('è™•ç†éŒ„éŸ³å¤±æ•—:', err)
    error.value = 'è™•ç†éŒ„éŸ³å¤±æ•—ï¼Œè«‹é‡è©¦'
    emit('error', error.value)
  } finally {
    isProcessing.value = false
    cleanup()
  }
}

async function uploadAudio(audioBlob: Blob) {
  const token = localStorage.getItem('token')
  if (!token) {
    throw new Error('æœªç™»å…¥')
  }
  
  const result = await speechApi.upload(props.roomId, audioBlob, props.userLang)
  console.log('âœ… STT æˆåŠŸ:', result)
  
  emit('transcript', {
    text: result.transcript,
    confidence: result.confidence,
    lang: result.detected_lang
  })
}

function getSupportedMimeType(): string {
  const types = [
    'audio/webm;codecs=opus',
    'audio/webm',
    'audio/mp4',
    'audio/ogg;codecs=opus',
    'audio/wav'
  ]
  
  return types.find(type => MediaRecorder.isTypeSupported(type)) || 'audio/webm'
}

function getFileExtension(): string {
  const mimeType = getSupportedMimeType()
  if (mimeType.includes('webm')) return 'webm'
  if (mimeType.includes('mp4')) return 'mp4'
  if (mimeType.includes('ogg')) return 'ogg'
  if (mimeType.includes('wav')) return 'wav'
  return 'webm'
}

function cleanup() {
  // é—œéµä¿®å¾©ï¼šä¸è¦åœæ­¢ streamï¼Œä¿æŒéº¥å…‹é¢¨æ¬Šé™æ´»èº
  // åªæ¸…ç†éŒ„éŸ³ç›¸é—œçš„è³‡æº
  
  stopVolumeAnalysis()
  
  if (recordingTimer.value) {
    clearInterval(recordingTimer.value)
    recordingTimer.value = null
  }
  
  if (vadTimer.value) {
    clearInterval(vadTimer.value)
    vadTimer.value = null
  }
  
  audioChunks.value = []
  recordingTime.value = 0
  silenceTimer.value = 0
  hasValidSpeech.value = false
  vadStatus.value = 'ç­‰å¾…èªéŸ³...'
  
  // é‡ç½®åˆ†æ®µç›¸é—œç‹€æ…‹
  segmentTimer.value = 0
  isSegmentMode.value = false
  hasProcessedSegment.value = false
}

// å®Œå…¨æ¸…ç†è³‡æºï¼ˆåƒ…åœ¨çµ„ä»¶å¸è¼‰æ™‚èª¿ç”¨ï¼‰
function fullCleanup() {
  if (stream.value) {
    stream.value.getTracks().forEach(track => track.stop())
    stream.value = null
  }
  cleanup()
  if (mediaRecorder.value) {
    mediaRecorder.value = null
  }
}

function formatTime(seconds: number): string {
  const mins = Math.floor(seconds / 60)
  const secs = Math.floor(seconds % 60)
  return `${mins}:${secs.toString().padStart(2, '0')}`
}
</script>

<style scoped>
.smart-voice-recorder {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 1rem;
  padding: 1rem;
}

.smart-record-btn {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  border: none;
  border-radius: 20px;
  padding: 1rem 2rem;
  cursor: pointer;
  font-size: 1rem;
  min-width: 200px;
  transition: all 0.3s ease;
  box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
  position: relative;
  overflow: hidden;
}

.smart-record-btn:hover:not(:disabled) {
  transform: translateY(-2px);
  box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
}

.smart-record-btn.recording {
  background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
  animation: recordingPulse 2s infinite;
}

.smart-record-btn.processing {
  background: linear-gradient(135deg, #f39c12 0%, #e67e22 100%);
}

.smart-record-btn.disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

@keyframes recordingPulse {
  0%, 100% { box-shadow: 0 4px 15px rgba(231, 76, 60, 0.3); }
  50% { box-shadow: 0 6px 25px rgba(231, 76, 60, 0.6); }
}

.btn-content {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 0.5rem;
}

.icon {
  font-size: 1.5rem;
}

.recording-animation {
  display: flex;
  align-items: center;
  justify-content: center;
  position: relative;
}

.pulse {
  position: absolute;
  width: 30px;
  height: 30px;
  border: 2px solid rgba(255, 255, 255, 0.6);
  border-radius: 50%;
  animation: pulse 1.5s infinite;
}

@keyframes pulse {
  0% {
    transform: scale(0.8);
    opacity: 1;
  }
  100% {
    transform: scale(2);
    opacity: 0;
  }
}

.text {
  text-align: center;
  line-height: 1.4;
}

.text small {
  font-size: 0.8rem;
  opacity: 0.9;
}

.smart-indicator {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 1rem;
  background: rgba(255, 255, 255, 0.95);
  padding: 1rem;
  border-radius: 12px;
  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
  min-width: 300px;
}

.volume-display {
  position: relative;
  width: 100%;
}

.volume-bars {
  display: flex;
  gap: 3px;
  align-items: end;
  justify-content: center;
  height: 40px;
}

.volume-bar {
  width: 6px;
  height: 8px;
  background: #e9ecef;
  border-radius: 3px;
  transition: all 0.1s;
}

.volume-bar.active {
  height: 20px;
}

.volume-bar.voice {
  background: linear-gradient(to top, #28a745, #20c997);
}

.volume-bar.silence {
  background: linear-gradient(to top, #6c757d, #adb5bd);
}

.threshold-line {
  position: absolute;
  top: 50%;
  width: 2px;
  height: 100%;
  background: #dc3545;
  transform: translateY(-50%);
  opacity: 0.7;
}

.threshold-line::before {
  content: 'é–¾å€¼';
  position: absolute;
  top: -20px;
  left: 50%;
  transform: translateX(-50%);
  font-size: 0.7rem;
  color: #dc3545;
  white-space: nowrap;
}

.vad-info {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 0.5rem;
  text-align: center;
}

.vad-status {
  font-weight: 600;
  padding: 0.5rem 1rem;
  border-radius: 20px;
  background: #f8f9fa;
  color: #6c757d;
  transition: all 0.3s;
}

.vad-status.active {
  background: #d4edda;
  color: #155724;
}

.silence-timer, .auto-stop-info {
  font-size: 0.8rem;
  color: #666;
}

.auto-stop-info {
  color: #e67e22;
  font-weight: 600;
}

.mode-selector {
  display: flex;
  gap: 0.5rem;
  background: #f8f9fa;
  padding: 0.25rem;
  border-radius: 20px;
}

.mode-btn {
  background: none;
  border: none;
  padding: 0.5rem 1rem;
  border-radius: 16px;
  cursor: pointer;
  font-size: 0.8rem;
  transition: all 0.3s;
  color: #666;
}

.mode-btn.active {
  background: #667eea;
  color: white;
  box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
}

.error-message {
  background: #f8d7da;
  color: #721c24;
  padding: 0.75rem;
  border-radius: 8px;
  text-align: center;
  font-size: 0.9rem;
  max-width: 300px;
}

/* éŸ¿æ‡‰å¼è¨­è¨ˆ */
@media (max-width: 768px) {
  .smart-voice-recorder {
    padding: 0.5rem;
  }
  
  .smart-record-btn {
    min-width: 180px;
    padding: 0.8rem 1.5rem;
  }
  
  .smart-indicator {
    min-width: 280px;
    padding: 0.8rem;
  }
  
  .volume-bars {
    height: 30px;
  }
  
  .volume-bar.active {
    height: 15px;
  }
}
</style>